---
title: Practical Machine Learning Project Report - Exercise Prediction
author: "by Michael Yap"
output:
  html_document:
    fig_height: 9
    fig_width: 9
---

## Introduction  
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement ? a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.  

In this report, we will show how to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict the manner in which they perform the exercise. The 5 possible methods include:

* A: exactly according to the specification
* B: throwing the elbows to the front
* C: lifting the dumbbell only halfway
* D: lowering the dumbbell only halfway
* E: throwing the hips to the front

## Data Preprocessing  
```{r, warning=FALSE}
library(lattice)
library(ggplot2)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(corrplot)
```
### Download the Data

The data can be downloaded from the following URLs:

#### Training:  
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

#### Test:      
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

In this project, it is assumed that the data files have been downloaded and placed in a subfolder './data'.

### Read the Data
Read the two CSV files into two data frames.
```{r, cache = T}
trainRaw <- read.csv("./data/pml-training.csv")
testRaw <- read.csv("./data/pml-testing.csv")
dim(trainRaw)
dim(testRaw)
colnames(trainRaw)
```
A simple data exploration shows that the training data set contains 19622 observations while the testing data set contains 20 observations. Both data set have 160 variables. The "classe" variable (refer to the last variable) in the training set is the outcome to predict. 

### Clean the data
After investigating all the variables of the sets, it's possible to see that there are quite a number of NA missing values, or useless or empty variables for the prediction. In this step, we will clean the data and get rid of observations with missing values as well as nonzero in the validation data set.
```{r, cache = T}
sum(complete.cases(trainRaw))
```
First, we remove columns that contain NA missing values.
```{r, cache = T}
trainRaw <- trainRaw[, colSums(is.na(trainRaw)) == 0] 
testRaw <- testRaw[, colSums(is.na(testRaw)) == 0] 
```  

Next, we remove some columns that do not contribute much to the accelerometer measurements.
```{r}
classe <- trainRaw$classe
trainRemove <- grepl("^X|timestamp|window", names(trainRaw))
trainRaw <- trainRaw[, !trainRemove]
trainCleaned <- trainRaw[, sapply(trainRaw, is.numeric)]
trainCleaned$classe <- classe
testRemove <- grepl("^X|timestamp|window", names(testRaw))
testRaw <- testRaw[, !testRemove]
testCleaned <- testRaw[, sapply(testRaw, is.numeric)]

dim(trainCleaned); 
dim(testCleaned)
```
Now, the cleaned training data set still contains 19622 observations and the testing data set contains 20 observations. But the number of variables has reduced to 53. The "classe" variable is still in the cleaned training set.

### Create Cross Validation data
Then, we can split the cleaned training set into a pure training data set (70%) and a validation data set (30%). We will use the validation data set to conduct cross validation in future steps.  
```{r}
set.seed(22520)
inTrain <- createDataPartition(trainCleaned$classe, p=0.70, list=F)
trainData <- trainCleaned[inTrain, ]
testData <- trainCleaned[-inTrain, ]
```

## Data Modeling
For prediction model, we are using **Random Forest** algorithm due to its highly accuracy rate. It automatically selects important variables and is robust to correlated covariates & outliers in general. We will use **5-fold cross validation** when applying the algorithm.  
```{r}
controlRf <- trainControl(method="cv", 5)
modelRf <- train(classe ~ ., data=trainData, method="rf", trControl=controlRf, ntree=250)
modelRf
```
Then, we estimate the performance of the model on the validation data set.  
```{r}
predictRf <- predict(modelRf, testData)
confusionMatrix(testData$classe, predictRf)
```

### Out of Sample Error
```{r}
accuracy <- postResample(predictRf, testData$classe)
accuracy
oosErr <- 1 - as.numeric(confusionMatrix(testData$classe, predictRf)$overall[1])
oosErr
```
As you can see, the estimated accuracy of the model is 99.56% and the estimated out-of-sample error is 0.44%.

## Predicting for Test Data Set
Now, we apply the model to the original testing data set. We remove the `problem_id` column first.  
```{r}
result <- predict(modelRf, testCleaned[, -length(names(testCleaned))])
result
```  

## Conclusion
The random forest model with cross validation produces a reasonably accurate model that is sufficient for predictive analytics.

## Appendix: Figure
1. Decision Tree Visualization
```{r}
treeModel <- rpart(classe ~ ., data=trainData, method="class")
prp(treeModel) # fast plot
```